
import argparse, os, re
from dataclasses import dataclass
from typing import Dict, List
import numpy as np, pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GroupKFold
from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor
from sklearn.linear_model import Ridge

RANDOM_SEED = int(os.environ.get("SEED", 2025))
np.random.seed(RANDOM_SEED)

def ensure_data_files(root: str) -> Dict[str, str]:
    req = {
        "clinical_train": os.path.join(root, "clinical_train.csv"),
        "molecular_train": os.path.join(root, "molecular_train.csv"),
        "target_train":   os.path.join(root, "target_train.csv"),
        "clinical_test":  os.path.join(root, "clinical_test.csv"),
        "molecular_test": os.path.join(root, "molecular_test.csv"),
    }
    miss = [k for k, p in req.items() if not os.path.exists(p)]
    if miss:
        raise FileNotFoundError(
            "Missing CSVs in data-root: " + ", ".join(miss) +
            "\nFix: put those files inside the folder passed to --data-root "
            "(use '.' if they are at repo root; default is ./data)."
        )
    return req

def rank_avg(arrs: List[np.ndarray]) -> np.ndarray:
    ranks = [pd.Series(a).rank(method="average").to_numpy() for a in arrs]
    return np.mean(ranks, axis=0)

def km_censoring(times: np.ndarray, events: np.ndarray):
    order = np.argsort(times)
    t = np.asarray(times)[order]
    d_c = (1 - np.asarray(events)[order]).astype(int)
    uniq = np.unique(t)
    n_at_risk = np.array([(t >= ut).sum() for ut in uniq], dtype=float)
    d_at = np.array([((t == ut) & (d_c == 1)).sum() for ut in uniq], dtype=float)
    G = np.ones_like(uniq, dtype=float)
    prod = 1.0
    for k in range(len(uniq)):
        if n_at_risk[k] > 0:
            prod *= (1.0 - d_at[k] / n_at_risk[k])
        G[k] = prod
    def G_hat(tq: np.ndarray) -> np.ndarray:
        idx = np.searchsorted(uniq, tq, side="right") - 1
        idx = np.clip(idx, 0, len(uniq)-1)
        return G[idx]
    return G_hat

def uno_c_index_ipcw(Ttr, Etr, Tva, Eva, Score, tau: float = 7.0) -> float:
    G_hat = km_censoring(np.asarray(Ttr), np.asarray(Etr))
    T = np.asarray(Tva).astype(float)
    E = np.asarray(Eva).astype(int)
    S = np.asarray(Score).astype(float)
    valid = np.where((E == 1) & (T <= tau))[0]
    if len(valid) == 0:
        return float("nan")
    num = 0.0; den = 0.0
    for i in valid:
        wi = 1.0 / max(G_hat(np.array([T[i]]))[0], 1e-8)
        mask = T >= T[i]
        den += wi * mask.sum()
        Sj = S[mask]
        num += wi * (np.sum(S[i] > Sj) + 0.5*np.sum(S[i] == Sj))
    return num/den if den > 0 else float("nan")

CYTO_PATTERNS = {
    "cyto_monosomy7": r"(?:^|[,;\s])-7(?!\d)",
    "cyto_del5q":     r"del\(5q\)",
    "cyto_trisomy8":  r"\+8(?!\d)",
    # remove capturing groups to avoid warnings
    "cyto_complex":   r"complex|(?:\()?\\d+\\s*abn\.",
}

def featurize_cytogenetics(s: pd.Series) -> pd.DataFrame:
    s = s.fillna("")
    out = pd.DataFrame(index=s.index)
    out["cyto_sex_xx"] = s.str.contains(r"46,\s*XX", case=False, regex=True).astype(int)
    out["cyto_sex_xy"] = s.str.contains(r"46,\s*XY", case=False, regex=True).astype(int)
    for name, rgx in CYTO_PATTERNS.items():
        out[name] = s.str.contains(rgx, case=True, regex=True).astype(int)
    out["cyto_abn_sepcount"] = s.str.count(r"[,;]").fillna(0)
    return out

@dataclass
class MolAgg:
    top_genes: int = 800
    top_effects: int = 36
    top_chr: int = 24
    top_pchange: int = 80
    genes_: List[str] = None
    effects_: List[str] = None
    chrs_: List[str] = None
    pchange_: List[str] = None

    def fit(self, df: pd.DataFrame):
        d = df.copy()
        d["GENE"] = d["GENE"].astype(str).fillna("unknown")
        self.genes_ = list(d["GENE"].value_counts().head(self.top_genes).index)
        self.effects_ = list(d.get("EFFECT", pd.Series([], dtype=str)).astype(str).fillna("unknown").value_counts().head(self.top_effects).index)
        self.chrs_ = list(d.get("CHR", pd.Series([], dtype=str)).astype(str).fillna("unknown").value_counts().head(self.top_chr).index)
        self.pchange_ = list(d.get("PROTEIN_CHANGE", pd.Series([], dtype=str)).astype(str).fillna("unknown").value_counts().head(self.top_pchange).index)
        return self

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        d = df.copy()
        d["GENE"] = d["GENE"].astype(str).fillna("unknown")
        if "VAF" in d.columns:
            d["VAF"] = pd.to_numeric(d["VAF"], errors="coerce")
        else:
            d["VAF"] = np.nan
        gene = (
            d[d["GENE"].isin(self.genes_)]
            .assign(present=1)
            .drop_duplicates(["ID","GENE"])
            .pivot_table(index="ID", columns="GENE", values="present", aggfunc="max", fill_value=0)
        )
        for g in self.genes_:
            if g not in gene.columns: gene[g] = 0
        gene = gene[self.genes_].add_prefix("gene_")
        if self.effects_ and "EFFECT" in d.columns:
            eff = d[d["EFFECT"].isin(self.effects_)].groupby(["ID","EFFECT"]).size().unstack(fill_value=0)
            for e in self.effects_:
                if e not in eff.columns: eff[e] = 0
            eff = eff[self.effects_].add_prefix("effect_")
        else:
            eff = pd.DataFrame(index=d["ID"].drop_duplicates())
        if self.chrs_ and "CHR" in d.columns:
            chrct = d[d["CHR"].astype(str).isin(self.chrs_)].groupby(["ID","CHR"]).size().unstack(fill_value=0)
            for c in self.chrs_:
                if c not in chrct.columns: chrct[c] = 0
            chrct = chrct[self.chrs_].add_prefix("chr_")
        else:
            chrct = pd.DataFrame(index=d["ID"].drop_duplicates())
        if self.pchange_ and "PROTEIN_CHANGE" in d.columns:
            pc = d[d["PROTEIN_CHANGE"].astype(str).isin(self.pchange_)].groupby(["ID","PROTEIN_CHANGE"]).size().unstack(fill_value=0)
            for p in self.pchange_:
                if p not in pc.columns: pc[p] = 0
            pc = pc[self.pchange_].add_prefix("pchg_")
        else:
            pc = pd.DataFrame(index=d["ID"].drop_duplicates())
        vaf = d.groupby("ID").agg(mut_count=("GENE","count"), vaf_mean=("VAF","mean"), vaf_max=("VAF","max"), vaf_std=("VAF","std")).fillna(0)
        out = gene.join([eff, chrct, pc, vaf], how="outer").fillna(0).reset_index()
        return out

def build_preprocessor(df: pd.DataFrame) -> ColumnTransformer:
    cols = [c for c in df.columns if c != "ID"]
    num_cols = [c for c in cols if df[c].dtype.kind in "fcui" and c != "CENTER"]
    cat_cols = [c for c in cols if df[c].dtype == object and c in {"CENTER"}]
    return ColumnTransformer(
        [
            ("num", Pipeline([("imp", SimpleImputer(strategy="median")), ("sc", StandardScaler())]), num_cols),
            # sklearn>=1.4 uses sparse_output instead of sparse
            ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")), ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))]), cat_cols),
        ],
        remainder="drop",
        verbose_feature_names_out=False,
    )

def train_and_submit(data_root: str, out_path: str, top_genes: int, top_effects: int, folds: int, tau: float) -> float:
    paths = ensure_data_files(data_root)
    clin_tr = pd.read_csv(paths["clinical_train"])
    mol_tr  = pd.read_csv(paths["molecular_train"])
    y_tr    = pd.read_csv(paths["target_train"])
    clin_te = pd.read_csv(paths["clinical_test"])
    mol_te  = pd.read_csv(paths["molecular_test"])

    agg = MolAgg(top_genes=top_genes, top_effects=top_effects).fit(mol_tr)
    mol_tr_f = agg.transform(mol_tr)
    mol_te_f = agg.transform(mol_te)

    X_train = clin_tr.merge(mol_tr_f, on="ID", how="left")
    X_test  = clin_te.merge(mol_te_f, on="ID", how="left")

    if "CYTOGENETICS" in X_train.columns:
        X_train = pd.concat([X_train, featurize_cytogenetics(X_train["CYTOGENETICS"])], axis=1)
    if "CYTOGENETICS" in X_test.columns:
        X_test = pd.concat([X_test, featurize_cytogenetics(X_test["CYTOGENETICS"])], axis=1)

    for c in X_train.columns:
        if c not in X_test.columns: X_test[c] = 0
    X_test = X_test[X_train.columns]

    # Clean target and align rows (drop invalid labels)
    y_clean = y_tr.copy()
    y_clean["OS_YEARS"] = pd.to_numeric(y_clean["OS_YEARS"], errors="coerce")
    y_clean["OS_STATUS"] = pd.to_numeric(y_clean["OS_STATUS"], errors="coerce").round().clip(0,1)
    Xy = X_train.merge(y_clean, on="ID", how="left")
    mask = Xy["OS_YEARS"].notna() & Xy["OS_STATUS"].isin([0,1])
    dropped = int((~mask).sum())
    if dropped: print(f"[CLEAN] Dropped {dropped} rows with missing/invalid OS labels.")
    X_train = Xy.loc[mask, X_train.columns]
    y = Xy.loc[mask, ["OS_YEARS","OS_STATUS"]].reset_index(drop=True)

    # Groups after filtering
    if "CENTER" in X_train.columns:
        X_train["CENTER"] = X_train["CENTER"].astype(str)
        X_test["CENTER"]  = X_test["CENTER"].astype(str)
        groups = X_train["CENTER"].astype(str)
    else:
        groups = pd.Series([0]*len(X_train))

    feat_cols = [c for c in X_train.columns if c != "ID"]
    Xtr = X_train[feat_cols]
    Xte = X_test[feat_cols]
    pre = build_preprocessor(X_train)

    gbr  = Pipeline([("pre", pre), ("reg", GradientBoostingRegressor(learning_rate=0.05, n_estimators=900, max_depth=3, random_state=RANDOM_SEED))])
    hgbr = Pipeline([("pre", pre), ("reg", HistGradientBoostingRegressor(learning_rate=0.06, max_depth=6, max_iter=900, random_state=RANDOM_SEED))])
    models = [("gbr", gbr), ("hgbr", hgbr)]

    T = y["OS_YEARS"].to_numpy().astype(float)
    E = y["OS_STATUS"].to_numpy().astype(int)
    T_tau = np.minimum(T, tau)
    y_pseudo = -T_tau.copy()
    y_pseudo[E == 0] *= 0.6

    n_groups = int(pd.Series(groups).nunique())
    n_splits = min(max(2, n_groups), max(2, folds))
    gkf = GroupKFold(n_splits=n_splits)

    oof = {name: np.zeros(len(Xtr)) for name, _ in models}
    te_preds = {name: [] for name, _ in models}
    scores = {name: [] for name, _ in models}

    for fold, (tr_idx, va_idx) in enumerate(gkf.split(Xtr, groups=groups)):
        X_tr, X_va = Xtr.iloc[tr_idx], Xtr.iloc[va_idx]
        T_tr, E_tr = T[tr_idx], E[tr_idx]
        T_va, E_va = T[va_idx], E[va_idx]
        G_hat = km_censoring(T_tr, E_tr)
        w_tr = 1.0 / np.maximum(G_hat(np.minimum(T_tr, tau)), 1e-8)
        w_tr[E_tr == 0] *= 0.5
        for name, m in models:
            m.fit(X_tr, y_pseudo[tr_idx], **{f"reg__sample_weight": w_tr})
            r_va = m.predict(X_va)
            cidx = float(uno_c_index_ipcw(T_tr, E_tr, T_va, E_va, r_va, tau=tau))
            scores[name].append(cidx)
            oof[name][va_idx] = r_va
            te_preds[name].append(m.predict(Xte))
            print(f"[Fold {fold+1}/{n_splits}] {name}: UnoC@{tau:.1f} = {cidx:.4f}")

    oof_blend = rank_avg([oof[n] for n, _ in models])
    test_blend = rank_avg([np.mean(np.vstack(te_preds[n]), axis=0) for n, _ in models])
    overall = float(uno_c_index_ipcw(T, E, T, E, oof_blend, tau=tau))
    print("[BLEND] OOF UnoC@{:.1f} = {:.4f} | by-model: {}".format(
        tau, overall, ", ".join(f"{k}:{np.mean(v):.4f}" for k, v in scores.items())
    ))

    Z_tr = np.column_stack([oof[n] for n, _ in models])
    Z_te = np.column_stack([np.mean(np.vstack(te_preds[n]), axis=0) for n, _ in models])
    stack = Ridge(alpha=1.0, random_state=RANDOM_SEED)
    stack.fit(Z_tr, y_pseudo)
    oof_stack = stack.predict(Z_tr)
    test_stack = stack.predict(Z_te)
    oof_final = rank_avg([oof_blend, oof_stack])
    test_final = rank_avg([test_blend, test_stack])
    overall_final = float(uno_c_index_ipcw(T, E, T, E, oof_final, tau=tau))
    print(f"[STACK] OOF UnoC@{tau:.1f} = {overall_final:.4f}")

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    sub = pd.DataFrame({"risk_score": test_final.astype(float)}, index=clin_te.set_index("ID").index)
    sub.index.name = "ID"
    sub.to_csv(out_path)
    print(f"[DONE] Submission written: {out_path}  rows={len(sub)}")
    return overall_final

def main():
    p = argparse.ArgumentParser(description="Train and write QRT submission (single-file)")
    p.add_argument("--data-root", default="data", help="Folder with the 5 CSVs. Use '.' if files are at repo root.")
    p.add_argument("--out", default="submissions/submission.csv", help="Output CSV path")
    p.add_argument("--top-genes", type=int, default=800)
    p.add_argument("--top-effects", type=int, default=36)
    p.add_argument("--folds", type=int, default=5)
    p.add_argument("--tau", type=float, default=7.0)
    a = p.parse_args()
    score = train_and_submit(a.data_root, a.out, a.top_genes, a.top_effects, a.folds, a.tau)
    print(f"[REPORT] OOF UnoC@{a.tau:.1f}: {score:.6f}")

if __name__ == "__main__":
    main()

